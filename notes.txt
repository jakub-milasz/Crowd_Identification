Transfer learning - ML technique where knowledge gained from solving one task is applied to a different but related task.

Firstly model is trained on a large dataset for a specific task
Next, pre-trained model is used as a starting point for a new, related task
Further, the model is trained on a smaller dataset specific to the target size, potentially adjusting some or all of it's layers

It is commonly used in Image Classification or NLP

Frozen layer - pre-trained layer which is blocked during training on the new dataset. The weights are not updated.
Trainable layer - those where weights are updated during the training

ExplainableAI (XAI) focuses on making AI systems understandable and transparent to humans. It helps understand the results given by machine learning alghorithms. We want to know how the model deduces and make decisions. In this way we can reveal potential biases in training data or model behaviour. Fo example, we can highlight important regions in images that influence a convolutional neural network's prediction.



XAI for my problem with CNN:

- Gradient-weighted Class Activation Mapping (Grad-CAM):
This technique generates heatmaps highlighting the regions in an input image that are most       influential in the CNN's prediction for a specific class. TensorFlow allows for easy computation of gradients with respect to feature maps, which is central to Grad-CAM.

- LIME (Local Interpretable Model-Agnostic Explanations):
While model-agnostic, LIME can be applied to CNNs to explain individual predictions by creating locally interpretable surrogate models.

- SHAP (SHapley Additive exPlanations):
Another model-agnostic method, SHAP provides a unified framework for interpreting predictions by attributing the contribution of each input feature to the final output.

- Feature Visualization:
Techniques like activation maximization can be used to visualize what patterns or features a specific neuron or layer in a CNN is responding to. 